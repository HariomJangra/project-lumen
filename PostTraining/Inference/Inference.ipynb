{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22f8f14f",
   "metadata": {},
   "source": [
    "### Importing Depenedencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d767ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from contextlib import nullcontext\n",
    "from torch.optim.lr_scheduler import LinearLR, SequentialLR, CosineAnnealingLR\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from tokenizers import Tokenizer\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "from ModelArchitecture import Transformer, ModelConfig, generate\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6100ccd",
   "metadata": {},
   "source": [
    "### Device Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c992fd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315d8482",
   "metadata": {},
   "source": [
    "### Importing Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2600304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded - Vocab size: 32,000\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer.from_file(\"LumenTokenizer.json\")\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(f\"Tokenizer loaded - Vocab size: {vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1e365d",
   "metadata": {},
   "source": [
    "### Initializing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "226b81a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig(\n",
    "    vocab_size=32000,          \n",
    "    hidden_size=768,           \n",
    "    n_heads=12,               \n",
    "    n_kv_heads=4,              \n",
    "    n_kv_groups=3,             \n",
    "    head_dim=64,              \n",
    "    n_layers=12,          \n",
    "    attention_bias=False,      \n",
    "    intermediate_size=3072,    \n",
    "    mlp_bias=False,            \n",
    "    eps=1e-5,                  \n",
    "    dropout=0.1,               \n",
    "    max_position_embeddings=2048,\n",
    "    pre_norm=True,             \n",
    "    tie_weights=True,\n",
    "    max_seq_len=2048\n",
    ")\n",
    "\n",
    "model = Transformer(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66992c8",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb3a558",
   "metadata": {},
   "source": [
    "#### SafeTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53bbc8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded .safetensors checkpoint: /Users/hariom/LLM Development/PostTraining/Models/best_sft_model_params.safetensors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (token_embedding): Embedding(32000, 768)\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (attention): GroupedMultiQueryAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
       "        (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (rope): RotaryEmbedding()\n",
       "      )\n",
       "      (feed_forward): SwiGLUFeedForward(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (gate_proj): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (up_proj): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (down_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): SiLU()\n",
       "      )\n",
       "      (attn_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (final_norm): RMSNorm()\n",
       "  (lm_head): Linear(in_features=768, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_path = \"/Users/hariom/LLM Development/PostTraining/Models/best_sft_model_params.safetensors\"\n",
    "state = load_file(weights_path, device=str(device))\n",
    "\n",
    "model.load_state_dict(state, strict=False)\n",
    "print(f\"Loaded .safetensors checkpoint: {weights_path}\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56b0004",
   "metadata": {},
   "source": [
    "#### .pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a47d3d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded .pt checkpoint: best_sft_model_latest.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (token_embedding): Embedding(32000, 768)\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (attention): GroupedMultiQueryAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
       "        (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (rope): RotaryEmbedding()\n",
       "      )\n",
       "      (feed_forward): SwiGLUFeedForward(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (gate_proj): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (up_proj): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (down_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): SiLU()\n",
       "      )\n",
       "      (attn_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (final_norm): RMSNorm()\n",
       "  (lm_head): Linear(in_features=768, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_path = \"best_sft_model_latest.pt\"\n",
    "state = torch.load(weights_path, map_location=device)\n",
    "\n",
    "model.load_state_dict(state, strict=False)\n",
    "print(f\"Loaded .pt checkpoint: {weights_path}\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8c79845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOS token ID: 6\n"
     ]
    }
   ],
   "source": [
    "eos_token = \"<|im_end|>\"\n",
    "eos_token_id = tokenizer.encode(eos_token).ids[0]\n",
    "print(f\"EOS token ID: {eos_token_id}\")\n",
    "\n",
    "def generate_response(prompt, max_tokens=200):\n",
    "    # Format prompt in chat format\n",
    "    formatted_prompt = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = tokenizer.encode(formatted_prompt).ids\n",
    "    input_ids = torch.tensor([tokens], dtype=torch.long, device=device)\n",
    "    \n",
    "    # Generate with EOS token stopping\n",
    "    with torch.no_grad():\n",
    "        output = generate(\n",
    "            model, \n",
    "            input_ids, \n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            eos_token_id=eos_token_id  # Stop at <|im_end|>\n",
    "        )\n",
    "    \n",
    "    # Decode full output\n",
    "    full_text = tokenizer.decode(output[0].tolist())\n",
    "    \n",
    "    # Extract only the assistant's response\n",
    "    if \"<|im_start|>assistant\" in full_text:\n",
    "        # Get text after the assistant marker\n",
    "        response_part = full_text.split(\"<|im_start|>assistant\")[-1]\n",
    "        # Remove the closing tag if present\n",
    "        if \"<|im_end|>\" in response_part:\n",
    "            response_part = response_part.split(\"<|im_end|>\")[0]\n",
    "        return response_part.strip()\n",
    "    \n",
    "    return full_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9fdad5",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eef7e5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Who created you?\n",
      "\n",
      "Response:\n",
      "user\n",
      "Who created you?\n",
      "assistant\n",
      "I’m an AI assistant named Lumen, created by Hariom Jangra from India.\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"Who created you?\"\n",
    "print(f\"Prompt: {test_prompt}\\n\")\n",
    "print(f\"Response:\\n{generate_response(test_prompt)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
