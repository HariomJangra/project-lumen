{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff76dbde",
   "metadata": {},
   "source": [
    "### Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63ec99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828f484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from contextlib import nullcontext\n",
    "from torch.optim.lr_scheduler import LinearLR, SequentialLR, CosineAnnealingLR\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from tokenizers import Tokenizer\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from ModelArchitecture import Transformer, ModelConfig\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6813f0ce",
   "metadata": {},
   "source": [
    "### Device Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08a5c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a8be7f",
   "metadata": {},
   "source": [
    "### Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b8cc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"LumenTokenizer.json\")\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(f\"Tokenizer loaded - Vocab size: {vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b171c60",
   "metadata": {},
   "source": [
    "### SFT Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9411e344",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SFTDataset(Dataset):\n",
    "    def __init__(self, jsonl_path, tokenizer, max_length=2048):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.data = []\n",
    "        \n",
    "        # Load JSONL data\n",
    "        with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    item = json.loads(line.strip())\n",
    "                    if 'text' in item:\n",
    "                        self.data.append(item['text'])\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "        \n",
    "        print(f\"Loaded {len(self.data)} samples from {jsonl_path}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer.encode(text)\n",
    "        tokens = encoding.ids\n",
    "        \n",
    "        # Truncate or pad\n",
    "        if len(tokens) > self.max_length:\n",
    "            tokens = tokens[:self.max_length]\n",
    "        \n",
    "        # Create input and target (shifted by 1)\n",
    "        input_ids = torch.tensor(tokens[:-1], dtype=torch.long)\n",
    "        target_ids = torch.tensor(tokens[1:], dtype=torch.long)\n",
    "        \n",
    "        return input_ids, target_ids\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle variable length sequences\"\"\"\n",
    "    input_ids = [item[0] for item in batch]\n",
    "    target_ids = [item[1] for item in batch]\n",
    "    \n",
    "    max_len = max(len(ids) for ids in input_ids)\n",
    "    \n",
    "    input_ids_padded = torch.zeros(len(batch), max_len, dtype=torch.long)\n",
    "    target_ids_padded = torch.zeros(len(batch), max_len, dtype=torch.long)\n",
    "    \n",
    "    for i, (inp, tgt) in enumerate(zip(input_ids, target_ids)):\n",
    "        input_ids_padded[i, :len(inp)] = inp\n",
    "        target_ids_padded[i, :len(tgt)] = tgt\n",
    "    \n",
    "    return input_ids_padded, target_ids_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a22a085",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a11a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_dataset = SFTDataset(\"Instruct_Dataset.jsonl\", tokenizer, max_length=2048)\n",
    "\n",
    "train_size = int(0.9 * len(sft_dataset))\n",
    "val_size = len(sft_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(sft_dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset):,}\")\n",
    "print(f\"Validation samples: {len(val_dataset):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab717f8e",
   "metadata": {},
   "source": [
    "### Model Configuration & Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10e59ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig(\n",
    "    vocab_size=32000,          \n",
    "    hidden_size=768,           \n",
    "    n_heads=12,               \n",
    "    n_kv_heads=4,              \n",
    "    n_kv_groups=3,             \n",
    "    head_dim=64,              \n",
    "    n_layers=12,          \n",
    "    attention_bias=False,      \n",
    "    intermediate_size=3072,    \n",
    "    mlp_bias=False,            \n",
    "    eps=1e-5,                  \n",
    "    dropout=0.1,               \n",
    "    max_position_embeddings=2048,\n",
    "    pre_norm=True,             \n",
    "    tie_weights=True,\n",
    "    max_seq_len=2048\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "model = Transformer(config)\n",
    "\n",
    "# Load pretrained weights\n",
    "pretrained_path = \"best_model_params.pt\"\n",
    "if os.path.exists(pretrained_path):\n",
    "    model.load_state_dict(torch.load(pretrained_path, map_location=device))\n",
    "    print(f\"✓ Loaded pretrained weights from {pretrained_path}\")\n",
    "else:\n",
    "    print(f\"Warning: Pretrained weights not found at {pretrained_path}\")\n",
    "    print(\"Starting from random initialization\")\n",
    "\n",
    "model = model.to(device)\n",
    "param_count = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model Parameters: {param_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba054df",
   "metadata": {},
   "source": [
    "### Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daea3967",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SFTConfig:\n",
    "    # Training\n",
    "    num_epochs: int = 5\n",
    "    batch_size: int = 12\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    eval_every_steps: int = 500\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate: float = 3e-5\n",
    "    warmup_steps: int = 400\n",
    "    min_lr: float = 1e-6\n",
    "    weight_decay: float = 0.01\n",
    "    grad_clip_norm: float = 1.0\n",
    "    betas: tuple = (0.9, 0.95)\n",
    "    \n",
    "    # Checkpointing\n",
    "    checkpoint_dir: str = \"sft_checkpoints\"\n",
    "    best_model_path: str = \"best_sft_model.pt\"\n",
    "    \n",
    "    # Mixed precision\n",
    "    use_amp: bool = True\n",
    "    dtype: str = \"bfloat16\" if torch.cuda.is_bf16_supported() else \"float16\"\n",
    "    \n",
    "    seed: int = 42\n",
    "\n",
    "sft_config = SFTConfig()\n",
    "\n",
    "# Setup precision\n",
    "device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "ptdtype = {\"float32\": torch.float32, \"bfloat16\": torch.bfloat16, \"float16\": torch.float16}[sft_config.dtype]\n",
    "ctx = nullcontext() if device_type == \"cpu\" else torch.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "print(f\"SFT Configuration:\")\n",
    "print(f\"  Epochs: {sft_config.num_epochs}\")\n",
    "print(f\"  Batch size: {sft_config.batch_size} (accumulation: {sft_config.gradient_accumulation_steps})\")\n",
    "print(f\"  Learning rate: {sft_config.learning_rate}\")\n",
    "print(f\"  Precision: {sft_config.dtype}\")\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(sft_config.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(sft_config.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(sft_config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4609e386",
   "metadata": {},
   "source": [
    "### DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dbbf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=sft_config.batch_size, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=sft_config.batch_size, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5666859e",
   "metadata": {},
   "source": [
    "### Optimizer & Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51705a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=sft_config.learning_rate,\n",
    "    betas=sft_config.betas,\n",
    "    weight_decay=sft_config.weight_decay\n",
    ")\n",
    "\n",
    "total_steps = len(train_loader) * sft_config.num_epochs // sft_config.gradient_accumulation_steps\n",
    "\n",
    "scheduler_warmup = LinearLR(optimizer, total_iters=sft_config.warmup_steps)\n",
    "scheduler_decay = CosineAnnealingLR(\n",
    "    optimizer, \n",
    "    T_max=total_steps - sft_config.warmup_steps, \n",
    "    eta_min=sft_config.min_lr\n",
    ")\n",
    "scheduler = SequentialLR(\n",
    "    optimizer, \n",
    "    [scheduler_warmup, scheduler_decay], \n",
    "    milestones=[sft_config.warmup_steps]\n",
    ")\n",
    "\n",
    "scaler = GradScaler('cuda', enabled=(sft_config.dtype == \"float16\"))\n",
    "\n",
    "print(f\"Total training steps: {total_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca463bf2",
   "metadata": {},
   "source": [
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c485b3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, max_batches=None):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for i, (input_ids, target_ids) in enumerate(dataloader):\n",
    "        if max_batches and i >= max_batches:\n",
    "            break\n",
    "            \n",
    "        input_ids = input_ids.to(device)\n",
    "        target_ids = target_ids.to(device)\n",
    "        \n",
    "        with ctx:\n",
    "            logits = model(input_ids)\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)), \n",
    "                target_ids.view(-1), \n",
    "                ignore_index=0  # Ignore padding\n",
    "            )\n",
    "        \n",
    "        total_loss += loss.item() * target_ids.numel()\n",
    "        total_tokens += target_ids.numel()\n",
    "    \n",
    "    model.train()\n",
    "    return total_loss / total_tokens if total_tokens > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecf3a90",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe36679",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "best_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "step = 0\n",
    "\n",
    "print(\"Starting Supervised Fine-Tuning...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(sft_config.num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{sft_config.num_epochs}\")\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\")\n",
    "    \n",
    "    for batch_idx, (input_ids, target_ids) in enumerate(progress_bar):\n",
    "        input_ids = input_ids.to(device)\n",
    "        target_ids = target_ids.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        with ctx:\n",
    "            logits = model(input_ids)\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)), \n",
    "                target_ids.view(-1),\n",
    "                ignore_index=0  # Ignore padding\n",
    "            )\n",
    "            loss = loss / sft_config.gradient_accumulation_steps\n",
    "        \n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Optimizer step\n",
    "        if (batch_idx + 1) % sft_config.gradient_accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), sft_config.grad_clip_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "            step += 1\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{optimizer.param_groups[0][\"lr\"]:.2e}'})\n",
    "        \n",
    "        # Evaluation\n",
    "        if step > 0 and step % sft_config.eval_every_steps == 0:\n",
    "            val_loss = evaluate(model, val_loader, max_batches=50)\n",
    "            train_losses.append(epoch_loss / (batch_idx + 1))\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            print(f\"\\nStep {step}: Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), sft_config.best_model_path)\n",
    "                print(f\"✓ Best model saved (val_loss: {best_val_loss:.4f})\")\n",
    "    \n",
    "    # End of epoch evaluation\n",
    "    val_loss = evaluate(model, val_loader)\n",
    "    print(f\"\\nEpoch {epoch + 1} Complete - Avg Train Loss: {epoch_loss / len(train_loader):.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    checkpoint_path = os.path.join(sft_config.checkpoint_dir, f\"checkpoint_epoch_{epoch + 1}.pt\")\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'val_loss': val_loss,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Complete!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a9ec9e",
   "metadata": {},
   "source": [
    "### Training Loss Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0f766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_losses and val_losses:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss', color='#4C78A8', linewidth=2)\n",
    "    plt.plot(val_losses, label='Validation Loss', color='#F58518', linewidth=2)\n",
    "    plt.xlabel('Evaluation Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('SFT Training Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No loss history available yet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
