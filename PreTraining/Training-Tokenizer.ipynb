{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e44d477",
   "metadata": {},
   "source": [
    "### Installing and Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3947e0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (0.21.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from tokenizers) (0.34.3)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.7.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d29a1bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tokenizers import Tokenizer, decoders\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents, Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1f0d28",
   "metadata": {},
   "source": [
    "### Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76c41824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a456e189231431992c8f80277c2b187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18defa98f57f44f88ea38c0a1a5c3ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"HariomJangra/Lumen-PreTraining\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c279b10b",
   "metadata": {},
   "source": [
    "### Training Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb46b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Tokenizer Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Tokenizer: 8976815it [38:20, 3902.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokenizer Training Completed!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(BPE())\n",
    "\n",
    "tokenizer.pre_tokenizer = ByteLevel(add_prefix_space=False)\n",
    "tokenizer.normalizer = Sequence([NFD(), StripAccents()])\n",
    "\n",
    "special_tokens=[\"<|endoftext|>\", \"<|system|>\", \"<|user|>\", \"<|assistant|>\", \"<|im_start|>\", \"<|im_sep|>\", \"<|im_end|>\", \"<think>\", \"</think>\", \"<pad>\", \"<unk>\"]\n",
    "\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=32000,\n",
    "    min_frequency= 2,\n",
    "    special_tokens = special_tokens\n",
    ")\n",
    "\n",
    "def text_iterator(dataset, text_key=\"text\"):\n",
    "    for sample in dataset:\n",
    "        if text_key in sample and sample[text_key]:\n",
    "            yield sample[text_key]\n",
    "\n",
    "iterator = text_iterator(dataset) \n",
    "\n",
    "print(\"Starting Tokenizer Training\")\n",
    "tokenizer.train_from_iterator(tqdm(iterator, desc=\"Training Tokenizer\", mininterval=5.0), trainer=trainer)\n",
    "print(\"Tokenizer Training Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fead30ca",
   "metadata": {},
   "source": [
    "### Save and Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8840892",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"Lumen-Tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29b9a79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"LumenTokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a190f6",
   "metadata": {},
   "source": [
    "### Testing Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "203133aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text\n",
      "<|user|>Hello AI! 👋\n",
      "Can you summarize the following text?\n",
      "\n",
      "“Artificial Intelligence (AI) is the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions.”\n",
      "\n",
      "Here are some points:\n",
      "\t1. AI includes machine learning, neural networks, and deep learning.\n",
      "\t2. AI can process natural language, images, and more.\n",
      "\t3. Check https://openai.com/ for reference.\n",
      "\n",
      "<|assistant|>Sure! In short, AI is about making machines think and act like humans.\n",
      "It involves learning, reasoning, and understanding complex inputs.\n",
      "<|endoftext|>\n",
      "\n",
      "Encoded Text\n",
      "IDs: [2, 15982, 19843, 11, 20537, 232, 226, 192, 8716, 621, 27696, 256, 1113, 3505, 41, 192, 192, 5057, 14349, 11781, 16004, 325, 30703, 19, 306, 256, 14506, 272, 3564, 10208, 274, 12491, 365, 406, 1176, 1418, 285, 3309, 1247, 9486, 283, 24229, 286, 579, 2568, 6158, 192, 192, 4588, 406, 792, 1497, 36, 192, 191, 27, 24, 19843, 2694, 6716, 5196, 22, 27897, 11584, 22, 283, 5742, 5196, 24, 192, 191, 28, 24, 19843, 534, 1429, 3442, 3354, 22, 8516, 22, 283, 786, 24, 192, 191, 29, 24, 1455, 13124, 4429, 12972, 1708, 24, 1259, 25, 314, 5903, 24, 1250, 3, 61, 459, 11, 402, 2059, 22, 19843, 306, 887, 2055, 12491, 3309, 283, 932, 1247, 9486, 24, 192, 2079, 6816, 5196, 22, 2978, 22, 283, 4973, 2810, 4546, 24, 192, 0]\n",
      "Tokens: ['<|user|>', 'Hello', 'ĠAI', '!', 'ĠðŁ', 'ĳ', 'ĭ', 'Ċ', 'Can', 'Ġyou', 'Ġsummarize', 'Ġthe', 'Ġfollowing', 'Ġtext', '?', 'Ċ', 'Ċ', 'âĢľ', 'Art', 'ificial', 'ĠIntelligence', 'Ġ(', 'AI', ')', 'Ġis', 'Ġthe', 'Ġsimulation', 'Ġof', 'Ġhuman', 'Ġintelligence', 'Ġin', 'Ġmachines', 'Ġthat', 'Ġare', 'Ġprogram', 'med', 'Ġto', 'Ġthink', 'Ġlike', 'Ġhumans', 'Ġand', 'Ġmim', 'ic', 'Ġtheir', 'Ġactions', '.âĢĿ', 'Ċ', 'Ċ', 'Here', 'Ġare', 'Ġsome', 'Ġpoints', ':', 'Ċ', 'ĉ', '1', '.', 'ĠAI', 'Ġincludes', 'Ġmachine', 'Ġlearning', ',', 'Ġneural', 'Ġnetworks', ',', 'Ġand', 'Ġdeep', 'Ġlearning', '.', 'Ċ', 'ĉ', '2', '.', 'ĠAI', 'Ġcan', 'Ġprocess', 'Ġnatural', 'Ġlanguage', ',', 'Ġimages', ',', 'Ġand', 'Ġmore', '.', 'Ċ', 'ĉ', '3', '.', 'ĠCheck', 'Ġhttps', '://', 'open', 'ai', '.', 'com', '/', 'Ġfor', 'Ġreference', '.', 'ĊĊ', '<|assistant|>', 'S', 'ure', '!', 'ĠIn', 'Ġshort', ',', 'ĠAI', 'Ġis', 'Ġabout', 'Ġmaking', 'Ġmachines', 'Ġthink', 'Ġand', 'Ġact', 'Ġlike', 'Ġhumans', '.', 'Ċ', 'It', 'Ġinvolves', 'Ġlearning', ',', 'Ġreasoning', ',', 'Ġand', 'Ġunderstanding', 'Ġcomplex', 'Ġinputs', '.', 'Ċ', '<|endoftext|>']\n",
      "\n",
      "Decoded Text (broken)\n",
      "<|user|>Hello AI! 👋\n",
      "Can you summarize the following text?\n",
      "\n",
      "“Artificial Intelligence (AI) is the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions.”\n",
      "\n",
      "Here are some points:\n",
      "\t1. AI includes machine learning, neural networks, and deep learning.\n",
      "\t2. AI can process natural language, images, and more.\n",
      "\t3. Check https://openai.com/ for reference.\n",
      "\n",
      "<|assistant|>Sure! In short, AI is about making machines think and act like humans.\n",
      "It involves learning, reasoning, and understanding complex inputs.\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "test_text = (\n",
    "    \"<|user|>Hello AI! 👋\\n\"\n",
    "    \"Can you summarize the following text?\\n\\n\"\n",
    "    \"“Artificial Intelligence (AI) is the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions.”\\n\\n\"\n",
    "    \"Here are some points:\\n\"\n",
    "    \"\\t1. AI includes machine learning, neural networks, and deep learning.\\n\"\n",
    "    \"\\t2. AI can process natural language, images, and more.\\n\"\n",
    "    \"\\t3. Check https://openai.com/ for reference.\\n\\n\"\n",
    "    \"<|assistant|>Sure! In short, AI is about making machines think and act like humans.\\n\"\n",
    "    \"It involves learning, reasoning, and understanding complex inputs.\\n\"\n",
    "    \"<|endoftext|>\"\n",
    ")\n",
    "\n",
    "print(\"Original Text\")\n",
    "print(test_text)\n",
    "\n",
    "encoded = tokenizer.encode(test_text)\n",
    "print(\"\\nEncoded Text\")\n",
    "print(\"IDs:\", encoded.ids)\n",
    "print(\"Tokens:\", encoded.tokens)\n",
    "\n",
    "decoded = tokenizer.decode(encoded.ids, skip_special_tokens=False)\n",
    "print(\"\\nDecoded Text (broken)\")\n",
    "print(decoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
