{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e44d477",
   "metadata": {},
   "source": [
    "### Installing and Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3947e0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (0.21.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from tokenizers) (0.34.3)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.7.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d29a1bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tokenizers import Tokenizer, decoders\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents, Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1f0d28",
   "metadata": {},
   "source": [
    "### Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76c41824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a456e189231431992c8f80277c2b187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18defa98f57f44f88ea38c0a1a5c3ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"HariomJangra/Lumen-PreTraining\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c279b10b",
   "metadata": {},
   "source": [
    "### Training Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb46b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Tokenizer Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Tokenizer: 8976815it [38:20, 3902.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokenizer Training Completed!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(BPE())\n",
    "\n",
    "tokenizer.pre_tokenizer = ByteLevel(add_prefix_space=False)\n",
    "tokenizer.normalizer = Sequence([NFD(), StripAccents()])\n",
    "\n",
    "special_tokens=[\"<|endoftext|>\", \"<|system|>\", \"<|user|>\", \"<|assistant|>\", \"<|im_start|>\", \"<|im_sep|>\", \"<|im_end|>\", \"<think>\", \"</think>\", \"<pad>\", \"<unk>\"]\n",
    "\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=32000,\n",
    "    min_frequency= 2,\n",
    "    special_tokens = special_tokens\n",
    ")\n",
    "\n",
    "def text_iterator(dataset, text_key=\"text\"):\n",
    "    for sample in dataset:\n",
    "        if text_key in sample and sample[text_key]:\n",
    "            yield sample[text_key]\n",
    "\n",
    "iterator = text_iterator(dataset) \n",
    "\n",
    "print(\"Starting Tokenizer Training\")\n",
    "tokenizer.train_from_iterator(tqdm(iterator, desc=\"Training Tokenizer\", mininterval=5.0), trainer=trainer)\n",
    "print(\"Tokenizer Training Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fead30ca",
   "metadata": {},
   "source": [
    "### Save and Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8840892",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"Lumen-Tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29b9a79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"LumenTokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a190f6",
   "metadata": {},
   "source": [
    "### Testing Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "203133aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text\n",
      "<|user|>Hello AI! ğŸ‘‹\n",
      "Can you summarize the following text?\n",
      "\n",
      "â€œArtificial Intelligence (AI) is the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions.â€\n",
      "\n",
      "Here are some points:\n",
      "\t1. AI includes machine learning, neural networks, and deep learning.\n",
      "\t2. AI can process natural language, images, and more.\n",
      "\t3. Check https://openai.com/ for reference.\n",
      "\n",
      "<|assistant|>Sure! In short, AI is about making machines think and act like humans.\n",
      "It involves learning, reasoning, and understanding complex inputs.\n",
      "<|endoftext|>\n",
      "\n",
      "Encoded Text\n",
      "IDs: [2, 15982, 19843, 11, 20537, 232, 226, 192, 8716, 621, 27696, 256, 1113, 3505, 41, 192, 192, 5057, 14349, 11781, 16004, 325, 30703, 19, 306, 256, 14506, 272, 3564, 10208, 274, 12491, 365, 406, 1176, 1418, 285, 3309, 1247, 9486, 283, 24229, 286, 579, 2568, 6158, 192, 192, 4588, 406, 792, 1497, 36, 192, 191, 27, 24, 19843, 2694, 6716, 5196, 22, 27897, 11584, 22, 283, 5742, 5196, 24, 192, 191, 28, 24, 19843, 534, 1429, 3442, 3354, 22, 8516, 22, 283, 786, 24, 192, 191, 29, 24, 1455, 13124, 4429, 12972, 1708, 24, 1259, 25, 314, 5903, 24, 1250, 3, 61, 459, 11, 402, 2059, 22, 19843, 306, 887, 2055, 12491, 3309, 283, 932, 1247, 9486, 24, 192, 2079, 6816, 5196, 22, 2978, 22, 283, 4973, 2810, 4546, 24, 192, 0]\n",
      "Tokens: ['<|user|>', 'Hello', 'Ä AI', '!', 'Ä Ã°Å', 'Ä³', 'Ä­', 'ÄŠ', 'Can', 'Ä you', 'Ä summarize', 'Ä the', 'Ä following', 'Ä text', '?', 'ÄŠ', 'ÄŠ', 'Ã¢Ä¢Ä¾', 'Art', 'ificial', 'Ä Intelligence', 'Ä (', 'AI', ')', 'Ä is', 'Ä the', 'Ä simulation', 'Ä of', 'Ä human', 'Ä intelligence', 'Ä in', 'Ä machines', 'Ä that', 'Ä are', 'Ä program', 'med', 'Ä to', 'Ä think', 'Ä like', 'Ä humans', 'Ä and', 'Ä mim', 'ic', 'Ä their', 'Ä actions', '.Ã¢Ä¢Ä¿', 'ÄŠ', 'ÄŠ', 'Here', 'Ä are', 'Ä some', 'Ä points', ':', 'ÄŠ', 'Ä‰', '1', '.', 'Ä AI', 'Ä includes', 'Ä machine', 'Ä learning', ',', 'Ä neural', 'Ä networks', ',', 'Ä and', 'Ä deep', 'Ä learning', '.', 'ÄŠ', 'Ä‰', '2', '.', 'Ä AI', 'Ä can', 'Ä process', 'Ä natural', 'Ä language', ',', 'Ä images', ',', 'Ä and', 'Ä more', '.', 'ÄŠ', 'Ä‰', '3', '.', 'Ä Check', 'Ä https', '://', 'open', 'ai', '.', 'com', '/', 'Ä for', 'Ä reference', '.', 'ÄŠÄŠ', '<|assistant|>', 'S', 'ure', '!', 'Ä In', 'Ä short', ',', 'Ä AI', 'Ä is', 'Ä about', 'Ä making', 'Ä machines', 'Ä think', 'Ä and', 'Ä act', 'Ä like', 'Ä humans', '.', 'ÄŠ', 'It', 'Ä involves', 'Ä learning', ',', 'Ä reasoning', ',', 'Ä and', 'Ä understanding', 'Ä complex', 'Ä inputs', '.', 'ÄŠ', '<|endoftext|>']\n",
      "\n",
      "Decoded Text (broken)\n",
      "<|user|>Hello AI! ğŸ‘‹\n",
      "Can you summarize the following text?\n",
      "\n",
      "â€œArtificial Intelligence (AI) is the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions.â€\n",
      "\n",
      "Here are some points:\n",
      "\t1. AI includes machine learning, neural networks, and deep learning.\n",
      "\t2. AI can process natural language, images, and more.\n",
      "\t3. Check https://openai.com/ for reference.\n",
      "\n",
      "<|assistant|>Sure! In short, AI is about making machines think and act like humans.\n",
      "It involves learning, reasoning, and understanding complex inputs.\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "test_text = (\n",
    "    \"<|user|>Hello AI! ğŸ‘‹\\n\"\n",
    "    \"Can you summarize the following text?\\n\\n\"\n",
    "    \"â€œArtificial Intelligence (AI) is the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions.â€\\n\\n\"\n",
    "    \"Here are some points:\\n\"\n",
    "    \"\\t1. AI includes machine learning, neural networks, and deep learning.\\n\"\n",
    "    \"\\t2. AI can process natural language, images, and more.\\n\"\n",
    "    \"\\t3. Check https://openai.com/ for reference.\\n\\n\"\n",
    "    \"<|assistant|>Sure! In short, AI is about making machines think and act like humans.\\n\"\n",
    "    \"It involves learning, reasoning, and understanding complex inputs.\\n\"\n",
    "    \"<|endoftext|>\"\n",
    ")\n",
    "\n",
    "print(\"Original Text\")\n",
    "print(test_text)\n",
    "\n",
    "encoded = tokenizer.encode(test_text)\n",
    "print(\"\\nEncoded Text\")\n",
    "print(\"IDs:\", encoded.ids)\n",
    "print(\"Tokens:\", encoded.tokens)\n",
    "\n",
    "decoded = tokenizer.decode(encoded.ids, skip_special_tokens=False)\n",
    "print(\"\\nDecoded Text (broken)\")\n",
    "print(decoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
