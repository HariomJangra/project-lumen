{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dfe52d9",
   "metadata": {},
   "source": [
    "### Installing and Importing necessary Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "877f61cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tokenizers import Tokenizer\n",
    "from ModelArchitecture import Transformer, ModelConfig, generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc510ca6",
   "metadata": {},
   "source": [
    "### Device Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15acba4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d47b2f",
   "metadata": {},
   "source": [
    "### Importing and Setting up Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c673f44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"LumenTokenizer.json\")\n",
    "\n",
    "def encode(text: str) -> torch.LongTensor:\n",
    "    return torch.tensor(tokenizer.encode(text).ids, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "def decode(ids: torch.LongTensor) -> str:\n",
    "    return tokenizer.decode(ids.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc0bf45",
   "metadata": {},
   "source": [
    "### Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ca96cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig(\n",
    "    vocab_size=32000,\n",
    "    hidden_size=768,\n",
    "    n_heads=12,\n",
    "    n_kv_heads=4,\n",
    "    n_kv_groups=3,\n",
    "    head_dim=64,\n",
    "    n_layers=12,\n",
    "    attention_bias=False,\n",
    "    intermediate_size=3072,\n",
    "    mlp_bias=False,\n",
    "    eps=1e-5,\n",
    "    dropout=0.0,\n",
    "    max_position_embeddings=2048,\n",
    "    pre_norm=True,\n",
    "    tie_weights=True,\n",
    "    max_seq_len=2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78bba75",
   "metadata": {},
   "source": [
    "### Initializing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74a37d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(config).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8fa67a",
   "metadata": {},
   "source": [
    "### Loading the PreTrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5af54982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded .pt checkpoint: ../Models/best_model_params_80k.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (token_embedding): Embedding(32000, 768)\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (attention): GroupedMultiQueryAttention(\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (k_proj): Linear(in_features=768, out_features=256, bias=False)\n",
       "        (v_proj): Linear(in_features=768, out_features=256, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (rope): RotaryEmbedding()\n",
       "      )\n",
       "      (feed_forward): SwiGLUFeedForward(\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (gate_proj): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (up_proj): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (down_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): SiLU()\n",
       "      )\n",
       "      (attn_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (embedding_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (final_norm): RMSNorm()\n",
       "  (lm_head): Linear(in_features=768, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_path = \"../Models/best_model_params_80k.pt\"\n",
    "state = torch.load(weights_path, map_location=device)\n",
    "\n",
    "# Handle state dict structure\n",
    "if isinstance(state, dict) and \"model_state_dict\" in state:\n",
    "    state = state[\"model_state_dict\"]\n",
    "\n",
    "model.load_state_dict(state, strict=False)\n",
    "print(f\"Loaded .pt checkpoint: {weights_path}\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1bbb62",
   "metadata": {},
   "source": [
    "### Generator Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ade3fe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_text(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 128,\n",
    "    temperature: float = 0.8,\n",
    "    top_k: int = 0,\n",
    "    top_p: float = 0.9,\n",
    "    do_sample: bool = True,\n",
    "    eos_token_id: int | None = None,\n",
    "    pad_token_id: int | None = None,\n",
    "):\n",
    "    input_ids = encode(prompt).to(device)\n",
    "    out_ids = generate(\n",
    "        model=model,\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        do_sample=do_sample,\n",
    "        eos_token_id=eos_token_id,\n",
    "        pad_token_id=pad_token_id,\n",
    "        device=device,\n",
    "    )\n",
    "    # Strip the prompt portion for the decoded continuation\n",
    "    continuation_ids = out_ids[0, input_ids.size(1):]\n",
    "    return decode(continuation_ids.cpu())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b76bfd",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbc7943a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "Once upon a time\n",
      "\n",
      "Generation:\n",
      ", in a land far away called Japan, there was a little girl named Maria. She loved to play with her toys and share them with her friends. But sometimes, she would feel sad or worried because she didn't know how to help her friends.\n",
      "\n",
      "Maria's mom explained to her that she had a special kind of helper called a therapist. She told Maria that she would talk to her parents and help her understand why she felt sad or worried. The therapist would listen carefully\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Once upon a time\"\n",
    "output = generate_text(prompt, max_new_tokens=100, temperature=0.7, top_p=0.8)\n",
    "print(\"Prompt:\")\n",
    "print(prompt)\n",
    "print(\"\\nGeneration:\")\n",
    "print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
