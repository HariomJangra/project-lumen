{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e44d477",
   "metadata": {},
   "source": [
    "### Installing and Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3947e0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29a1bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tokenizers import Tokenizer, decoders\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents, Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1f0d28",
   "metadata": {},
   "source": [
    "### Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c41824",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"YourAccountName/PreTraining\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c279b10b",
   "metadata": {},
   "source": [
    "### Training Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb46b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE())\n",
    "\n",
    "tokenizer.pre_tokenizer = ByteLevel(add_prefix_space=False)\n",
    "tokenizer.normalizer = Sequence([NFD(), StripAccents()])\n",
    "\n",
    "special_tokens=[\"<|endoftext|>\", \"<|system|>\", \"<|user|>\", \"<|assistant|>\", \"<|im_start|>\", \"<|im_sep|>\", \"<|im_end|>\", \"<think>\", \"</think>\", \"<pad>\", \"<unk>\"]\n",
    "\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=32000,\n",
    "    min_frequency= 2,\n",
    "    special_tokens = special_tokens\n",
    ")\n",
    "\n",
    "def text_iterator(dataset, text_key=\"text\"):\n",
    "    for sample in dataset:\n",
    "        if text_key in sample and sample[text_key]:\n",
    "            yield sample[text_key]\n",
    "\n",
    "iterator = text_iterator(dataset) \n",
    "\n",
    "print(\"Starting Tokenizer Training\")\n",
    "tokenizer.train_from_iterator(tqdm(iterator, desc=\"Training Tokenizer\", mininterval=5.0), trainer=trainer)\n",
    "print(\"Tokenizer Training Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fead30ca",
   "metadata": {},
   "source": [
    "### Save and Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c12d795",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"LumenTokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b9a79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"LumenTokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a190f6",
   "metadata": {},
   "source": [
    "### Testing Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203133aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = (\n",
    "    \"<|user|>Hello there! üëã\\n\"\n",
    "    \"Can you summarize the following passage?\\n\\n\"\n",
    "    \"‚ÄúArtificial intelligence refers to the ability of machines to perform tasks that normally require human intelligence, such as learning and problem-solving.‚Äù\\n\\n\"\n",
    "    \"Here are some points:\\n\"\n",
    "    \"\\t1. It involves learning algorithms and neural networks.\\n\"\n",
    "    \"\\t2. It can process language, images, and patterns.\\n\"\n",
    "    \"\\t3. It aims to make systems more adaptive and intelligent.\\n\\n\"\n",
    "    \"<|assistant|>Sure! In short, artificial intelligence is about enabling machines to learn and think like humans.\\n\"\n",
    "    \"It focuses on reasoning, understanding, and decision-making.\\n\"\n",
    "    \"<|endoftext|>\"\n",
    ")\n",
    "\n",
    "print(\"Original Text\")\n",
    "print(test_text)\n",
    "\n",
    "encoded = tokenizer.encode(test_text)\n",
    "print(\"\\nEncoded Text\")\n",
    "print(\"IDs:\", encoded.ids)\n",
    "print(\"Tokens:\", encoded.tokens)\n",
    "\n",
    "decoded = tokenizer.decode(encoded.ids, skip_special_tokens=False)\n",
    "print(\"\\nDecoded Text\")\n",
    "print(decoded)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
