{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e44d477",
   "metadata": {},
   "source": [
    "### Installing and Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3947e0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d29a1bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tokenizers import Tokenizer, decoders\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents, Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1f0d28",
   "metadata": {},
   "source": [
    "### Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c41824",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"YourAccountName/PreTraining\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c279b10b",
   "metadata": {},
   "source": [
    "### Training Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb46b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE())\n",
    "\n",
    "tokenizer.pre_tokenizer = ByteLevel(add_prefix_space=False)\n",
    "tokenizer.normalizer = Sequence([NFD(), StripAccents()])\n",
    "\n",
    "special_tokens=[\"<|endoftext|>\", \"<|system|>\", \"<|user|>\", \"<|assistant|>\", \"<|im_start|>\", \"<|im_sep|>\", \"<|im_end|>\", \"<think>\", \"</think>\", \"<pad>\", \"<unk>\"]\n",
    "\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=32000,\n",
    "    min_frequency= 2,\n",
    "    special_tokens = special_tokens\n",
    ")\n",
    "\n",
    "def text_iterator(dataset, text_key=\"text\"):\n",
    "    for sample in dataset:\n",
    "        if text_key in sample and sample[text_key]:\n",
    "            yield sample[text_key]\n",
    "\n",
    "iterator = text_iterator(dataset) \n",
    "\n",
    "print(\"Starting Tokenizer Training\")\n",
    "tokenizer.train_from_iterator(tqdm(iterator, desc=\"Training Tokenizer\", mininterval=5.0), trainer=trainer)\n",
    "print(\"Tokenizer Training Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fead30ca",
   "metadata": {},
   "source": [
    "### Save and Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c12d795",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"LumenTokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b9a79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"LumenTokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a190f6",
   "metadata": {},
   "source": [
    "### Testing Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66a4892d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text\n",
      "<|im_start|>user\n",
      "What are the three primary colors?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The three primary colors are red, blue, and yellow.<|im_end|>\n",
      "\n",
      "Encoded Text\n",
      "IDs: [4, 3396, 192, 3340, 406, 256, 874, 4180, 10688, 41, 6, 192, 4, 558, 8508, 192, 578, 874, 4180, 10688, 406, 2987, 22, 5091, 22, 283, 6870, 24, 6]\n",
      "Tokens: ['<|im_start|>', 'user', 'Ċ', 'What', 'Ġare', 'Ġthe', 'Ġthree', 'Ġprimary', 'Ġcolors', '?', '<|im_end|>', 'Ċ', '<|im_start|>', 'ass', 'istant', 'Ċ', 'The', 'Ġthree', 'Ġprimary', 'Ġcolors', 'Ġare', 'Ġred', ',', 'Ġblue', ',', 'Ġand', 'Ġyellow', '.', '<|im_end|>']\n",
      "\n",
      "Decoded Text\n",
      "<|im_start|>user\n",
      "What are the three primary colors?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The three primary colors are red, blue, and yellow.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "test_text = \"<|im_start|>user\\nWhat are the three primary colors?<|im_end|>\\n<|im_start|>assistant\\nThe three primary colors are red, blue, and yellow.<|im_end|>\"\n",
    "\n",
    "print(\"Original Text\")\n",
    "print(test_text)\n",
    "\n",
    "encoded = tokenizer.encode(test_text)\n",
    "print(\"\\nEncoded Text\")\n",
    "print(\"IDs:\", encoded.ids)\n",
    "print(\"Tokens:\", encoded.tokens)\n",
    "\n",
    "decoded = tokenizer.decode(encoded.ids, skip_special_tokens=False)\n",
    "print(\"\\nDecoded Text\")\n",
    "print(decoded)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
