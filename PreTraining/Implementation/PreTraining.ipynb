{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82f582d5",
   "metadata": {},
   "source": [
    "### Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65ce10df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from contextlib import nullcontext\n",
    "from torch.optim.lr_scheduler import LinearLR, SequentialLR, CosineAnnealingLR\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tokenizers import Tokenizer\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "from ModelArchitecture import Transformer, ModelConfig\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cdc175",
   "metadata": {},
   "source": [
    "### Device Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac640a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5272965",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e88f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = np.load(\"train_split.npy\")\n",
    "val_split   = np.load(\"val_split.npy\")\n",
    "\n",
    "print(\"Training Data:\", train_split.shape)\n",
    "print(\"Validation Data:\", val_split.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4b8bf1",
   "metadata": {},
   "source": [
    "### Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f657bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig(\n",
    "    vocab_size=32000,          \n",
    "    hidden_size=768,           \n",
    "    n_heads=12,               \n",
    "    n_kv_heads=4,              \n",
    "    n_kv_groups=3,             \n",
    "    head_dim=64,              \n",
    "    n_layers=12,          \n",
    "    attention_bias=False,      \n",
    "    intermediate_size=3072,    \n",
    "    mlp_bias=False,            \n",
    "    eps=1e-5,                  \n",
    "    dropout=0.1,               \n",
    "    max_position_embeddings=2048,\n",
    "    pre_norm=True,             \n",
    "    tie_weights=True,\n",
    "    max_seq_len=2048\n",
    ")\n",
    "\n",
    "print(f\"Model Configuration:\")\n",
    "print(f\"  Architecture: {config.hidden_size}d, {config.n_layers}L, {config.n_heads}H, {config.intermediate_size}ff\")\n",
    "print(f\"  Vocabulary: {config.vocab_size:,} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71134b10",
   "metadata": {},
   "source": [
    "### Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85d0cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(config)\n",
    "model = model.to(device)\n",
    "\n",
    "parameter_count = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model Device: {device}\")\n",
    "print(f\"Total Parameters: {parameter_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d436bf",
   "metadata": {},
   "source": [
    "### Training Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c46beca",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    learning_rate: float = 3e-4\n",
    "    warmup_steps: int = 200\n",
    "    min_lr: float = 1e-4\n",
    "    max_steps: int = 50000\n",
    "    eval_every: int = 5000\n",
    "    eval_steps: int = 100\n",
    "    \n",
    "    # Batch and sequence settings\n",
    "    batch_size: int = 12\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    \n",
    "    weight_decay: float = 0.1\n",
    "    grad_clip_norm: float = 0.5\n",
    "    betas: tuple = (0.9, 0.95)\n",
    "    eps: float = 1e-9\n",
    "    \n",
    "    checkpoint_every: int = 5000       \n",
    "    checkpoint_dir: str = \"checkpoints\"  \n",
    "    max_checkpoints: int = 5           \n",
    "    best_model_path: str = \"best_model_params.pt\"\n",
    "    \n",
    "    use_amp: bool = True\n",
    "    dtype: str = \"bfloat16\"  \n",
    "    \n",
    "    seed: int = 42\n",
    "\n",
    "train_config = TrainingConfig()\n",
    "\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"  Optimization: lr={train_config.learning_rate}, warmup={train_config.warmup_steps}\")\n",
    "print(f\"  Batch: size={train_config.batch_size}, accumulation={train_config.gradient_accumulation_steps}\")\n",
    "print(f\"  Training: {train_config.max_steps} steps, eval every {train_config.eval_every}\")\n",
    "print(f\"  Checkpointing: Every {train_config.checkpoint_every} steps to '{train_config.checkpoint_dir}'\")\n",
    "\n",
    "# Device setup (use existing device from earlier cell)\n",
    "device_type = \"cuda\" if \"cuda\" in str(device) else \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(0)\n",
    "\n",
    "# Precision - auto-detect best available\n",
    "if torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n",
    "    train_config.dtype = \"bfloat16\"\n",
    "elif torch.cuda.is_available():\n",
    "    train_config.dtype = \"float16\"\n",
    "else:\n",
    "    train_config.dtype = \"float32\"\n",
    "\n",
    "ptdtype = {\n",
    "    \"float32\": torch.float32,\n",
    "    \"bfloat16\": torch.bfloat16,\n",
    "    \"float16\": torch.float16,\n",
    "}[train_config.dtype]\n",
    "# Use proper autocast API signature\n",
    "ctx = nullcontext() if device_type == \"cpu\" else torch.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "print(f\"  Precision: {train_config.dtype}\")\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(train_config.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(train_config.seed)\n",
    "    torch.cuda.manual_seed_all(train_config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae0b14a",
   "metadata": {},
   "source": [
    "### Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c151645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=train_config.learning_rate,\n",
    "    betas=train_config.betas,\n",
    "    weight_decay=train_config.weight_decay,\n",
    "    eps=train_config.eps,\n",
    ")\n",
    "scheduler_warmup = LinearLR(optimizer, total_iters=train_config.warmup_steps)\n",
    "scheduler_decay = CosineAnnealingLR(\n",
    "    optimizer, \n",
    "    T_max=train_config.max_steps - train_config.warmup_steps, \n",
    "    eta_min=train_config.min_lr\n",
    ")\n",
    "scheduler = SequentialLR(\n",
    "    optimizer, \n",
    "    [scheduler_warmup, scheduler_decay], \n",
    "    milestones=[train_config.warmup_steps]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c0adec",
   "metadata": {},
   "source": [
    "### Mixed Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ba05a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.cuda.amp.GradScaler(enabled=(train_config.dtype == \"float16\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebc8136",
   "metadata": {},
   "source": [
    "### Batching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d55f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_split if split == 'train' else val_split\n",
    "    # Use model config for sequence length and training config for batch size\n",
    "    ix = torch.randint(len(data) - config.max_seq_len, (train_config.batch_size,))\n",
    "    idx_list = ix.tolist()  # convert to Python ints for numpy slicing\n",
    "    x = torch.stack([torch.from_numpy(data[i:i+config.max_seq_len]).long() for i in idx_list])\n",
    "    y = torch.stack([torch.from_numpy(data[i+1:i+1+config.max_seq_len]).long() for i in idx_list])\n",
    "\n",
    "    if device.type == 'cuda':\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e328e435",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ce3372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss(model, eval_iters=100):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for split in ['train', 'val']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                X, Y = get_batch(split)\n",
    "                logits = model(X)\n",
    "                # Compute cross-entropy loss - ensure target is long type\n",
    "                # Reshape for cross-entropy: (batch_size * seq_len, vocab_size) and (batch_size * seq_len)\n",
    "                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), Y.view(-1).long())\n",
    "                losses[k] = loss.item()\n",
    "            out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42be6df5",
   "metadata": {},
   "source": [
    "### Resume Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9586f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Specify which checkpoint to resume from\n",
    "resume_from_step = 0 # ← Change this to your checkpoint step (1000, 2000, 3000, etc.)\n",
    "\n",
    "#Load the checkpoint (only if resuming)\n",
    "if resume_from_step > 0:\n",
    "    checkpoint_path = f\"{train_config.checkpoint_dir}/checkpoint_step_{resume_from_step}.pt\"\n",
    "    print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    # STEP 3: Restore everything from checkpoint\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "    # STEP 4: Restore training variables\n",
    "    best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "    train_loss_list = checkpoint.get('train_loss_list', [])\n",
    "    validation_loss_list = checkpoint.get('validation_loss_list', [])\n",
    "    evaluation_steps = checkpoint.get('evaluation_steps', [])\n",
    "\n",
    "    # STEP 5: Set where to start\n",
    "    start_step = checkpoint['step']\n",
    "    print(f\"Resumed from step {start_step}\")\n",
    "else:\n",
    "    best_val_loss = float('inf')\n",
    "    train_loss_list, validation_loss_list, evaluation_steps = [], [], []\n",
    "    start_step = 0\n",
    "\n",
    "# Set your new target steps\n",
    "train_config.max_steps = 10000\n",
    "print(f\"Best validation loss so far: {best_val_loss if best_val_loss != float('inf') else 'N/A'}\")\n",
    "print(f\"Training will continue from step {start_step + 1} to step {train_config.max_steps}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bb021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "os.makedirs(train_config.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(train_config.seed)\n",
    "    torch.cuda.set_device(0)\n",
    "    torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "train_loss_list = list(train_loss_list)\n",
    "validation_loss_list = list(validation_loss_list)\n",
    "evaluation_steps = list(evaluation_steps)\n",
    "if not evaluation_steps and train_loss_list:\n",
    "    evaluation_steps = [train_config.eval_every * (i + 1) for i in range(len(train_loss_list))]\n",
    "history_path = os.path.join(train_config.checkpoint_dir, \"loss_history.pt\")\n",
    "\n",
    "print(f\"Starting training for {train_config.max_steps - start_step} more iterations...\")\n",
    "print(f\"Model: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "print(f\"Batch size: {train_config.batch_size}, Sequence length: {config.max_seq_len}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for step in tqdm(range(start_step + 1, train_config.max_steps + 1)):\n",
    "    if step % train_config.eval_every == 0 and step != start_step + 1:\n",
    "        losses = estimate_loss(model, train_config.eval_steps)\n",
    "        current_train_loss = float(losses['train'])\n",
    "        current_val_loss = float(losses['val'])\n",
    "        train_loss_list.append(current_train_loss)\n",
    "        validation_loss_list.append(current_val_loss)\n",
    "        evaluation_steps.append(step)\n",
    "\n",
    "        print(f\"Step {step}: train loss {current_train_loss:.4f}, val loss {current_val_loss:.4f}\")\n",
    "        print(f\"The current learning rate: {optimizer.param_groups[0]['lr']:.5f}\")\n",
    "\n",
    "        if current_val_loss < best_val_loss:\n",
    "            best_val_loss = current_val_loss\n",
    "            torch.save(model.state_dict(), train_config.best_model_path)\n",
    "            print(f\"New best model saved with val loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    if step % train_config.checkpoint_every == 0 and step != start_step + 1:\n",
    "        checkpoint_path = f\"{train_config.checkpoint_dir}/checkpoint_step_{step}.pt\"\n",
    "        torch.save({\n",
    "            'step': step,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'train_loss_list': train_loss_list,\n",
    "            'validation_loss_list': validation_loss_list,\n",
    "            'evaluation_steps': evaluation_steps\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "    X, y = get_batch(\"train\")\n",
    "\n",
    "    with ctx:\n",
    "        logits = model(X)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1).long())\n",
    "        loss = loss / train_config.gradient_accumulation_steps\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "    if ((step % train_config.gradient_accumulation_steps) == 0) or (step == train_config.max_steps):\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=train_config.grad_clip_norm)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Final step: {train_config.max_steps}\")\n",
    "\n",
    "history = {\n",
    "    'steps': evaluation_steps,\n",
    "    'train': train_loss_list,\n",
    "    'val': validation_loss_list,\n",
    "    'best_val_loss': best_val_loss,\n",
    "    'max_steps': train_config.max_steps\n",
    "}\n",
    "torch.save(history, history_path)\n",
    "print(f\"Loss history saved to {history_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8826c6fa",
   "metadata": {},
   "source": [
    "### Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d63180",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_path = os.path.join(train_config.checkpoint_dir, \"loss_history.pt\")\n",
    "\n",
    "if os.path.exists(history_path):\n",
    "    history = torch.load(history_path, map_location='cpu')\n",
    "    steps = list(history.get('steps', []))\n",
    "    train_curve = [float(x) for x in history.get('train', [])]\n",
    "    val_curve = [float(x) for x in history.get('val', [])]\n",
    "    max_steps = int(history.get('max_steps', train_config.max_steps))\n",
    "else:\n",
    "    steps = list(evaluation_steps) if 'evaluation_steps' in globals() else []\n",
    "    train_curve = [float(x) for x in train_loss_list] if 'train_loss_list' in globals() else []\n",
    "    val_curve = [float(x) for x in validation_loss_list] if 'validation_loss_list' in globals() else []\n",
    "    max_steps = int(train_config.max_steps)\n",
    "\n",
    "if steps and len(steps) == len(train_curve) == len(val_curve):\n",
    "    ordered = sorted(zip(steps, train_curve, val_curve))\n",
    "    steps, train_curve, val_curve = map(list, zip(*ordered))\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(steps, train_curve, label='Train loss', color='#4C78A8')\n",
    "    plt.plot(steps, val_curve, label='Validation loss', color='#F58518')\n",
    "    plt.xlabel('Training step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training vs Validation Loss')\n",
    "    plt.xlim(0, max_steps)\n",
    "    plt.grid(alpha=0.2)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "else:\n",
    "    print(\"No loss history available yet. Run training to populate the history.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b755e5",
   "metadata": {},
   "source": [
    "### Conversion to SafeTensor (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915ace93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted ../Models/best_model_params.pt → best_model_params.safetensors\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import save_file\n",
    "\n",
    "pt_file = \"../Models/best_model_params.pt\"\n",
    "safetensors_file = \"best_model_params.safetensors\"\n",
    "\n",
    "state_dict = torch.load(pt_file, map_location='cpu')\n",
    "\n",
    "# Clone all tensors to avoid shared memory issues\n",
    "tensors_to_save = {key: tensor.clone() for key, tensor in state_dict.items()}\n",
    "\n",
    "save_file(tensors_to_save, safetensors_file)\n",
    "print(f\"Converted {pt_file} → {safetensors_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
