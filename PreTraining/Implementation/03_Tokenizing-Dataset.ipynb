{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0972cf7",
   "metadata": {},
   "source": [
    "### Importing and Installing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfca9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7560e5e",
   "metadata": {},
   "source": [
    "### Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bf18e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"YourAccountName/PreTraining\", split=\"\", streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827776d0",
   "metadata": {},
   "source": [
    "### Load Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c971dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"LumenTokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c4634a",
   "metadata": {},
   "source": [
    "### Tokenizing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17edd8c7",
   "metadata": {},
   "source": [
    "#### Tokenizing in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea6c09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size_samples = 2_000_000 \n",
    "chunk_counter = 0\n",
    "eos_id = tokenizer.encode(\"<|endoftext|>\", add_special_tokens=False).ids[0]\n",
    "\n",
    "current_chunk = []\n",
    "current_samples = 0\n",
    "\n",
    "for item in tqdm(dataset, desc=\"Tokenizing\"):\n",
    "    tokens = tokenizer.encode(item[\"text\"], add_special_tokens=False).ids\n",
    "    current_chunk.extend(tokens)\n",
    "    current_chunk.append(eos_id)\n",
    "    \n",
    "    current_samples += 1\n",
    "    if current_samples >= chunk_size_samples:\n",
    "        np.save(f\"tokens_chunk_{chunk_counter}.npy\", np.array(current_chunk, dtype=np.int32))\n",
    "        print(f\"Saved chunk {chunk_counter} with {len(current_chunk)} tokens ({current_samples} samples)\")\n",
    "        chunk_counter += 1\n",
    "        current_chunk = []\n",
    "        current_samples = 0\n",
    "\n",
    "if current_chunk:\n",
    "    np.save(f\"tokens_chunk_{chunk_counter}.npy\", np.array(current_chunk, dtype=np.int32))\n",
    "    print(f\"Saved final chunk {chunk_counter} with {len(current_chunk)} tokens ({current_samples} samples)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d72b145",
   "metadata": {},
   "source": [
    "#### Merging Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3605ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_to_merge = [\"tokens_chunk_0.npy\"]\n",
    "\n",
    "all_tokens = []\n",
    "\n",
    "# Load and append each chunk\n",
    "for chunk_file in chunks_to_merge:\n",
    "    arr = np.load(chunk_file)\n",
    "    all_tokens.append(arr)\n",
    "\n",
    "# Concatenate all tokens into one array\n",
    "merged_tokens = np.concatenate(all_tokens, axis=0)\n",
    "\n",
    "# Save as a new merged chunk\n",
    "np.save(\"TokenizedDataSet.npy\", merged_tokens)\n",
    "print(f\"Merged chunk saved with {len(merged_tokens)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf975522",
   "metadata": {},
   "source": [
    "### Train and Validation Splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71502032",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = np.load(\"TokenizedDataSet.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1495d054",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = int(0.9*len(all_tokens))\n",
    "train_split = all_tokens[:split_ratio]\n",
    "val_split = all_tokens[split_ratio:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d78c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving Training and Validation Data\")\n",
    "np.save(\"train_split.npy\", train_split)\n",
    "np.save(\"val_split.npy\", val_split)\n",
    "print(\"Training and Validation Data Saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
