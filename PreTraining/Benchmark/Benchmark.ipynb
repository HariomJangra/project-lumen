{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53a56887",
   "metadata": {},
   "source": [
    "### Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19ddcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "from ModelArchitecture import Transformer, ModelConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d63f4e4",
   "metadata": {},
   "source": [
    "### Device "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e33263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc7a8c9",
   "metadata": {},
   "source": [
    "###  Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e674f957",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"LumenTokenizer.json\")\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "\n",
    "config = ModelConfig(\n",
    "    vocab_size=32000,\n",
    "    hidden_size=768,\n",
    "    n_heads=12,\n",
    "    n_kv_heads=4,\n",
    "    n_kv_groups=3,\n",
    "    head_dim=64,\n",
    "    n_layers=12,\n",
    "    attention_bias=False,\n",
    "    intermediate_size=3072,\n",
    "    mlp_bias=False,\n",
    "    eps=1e-5,\n",
    "    dropout=0.0,\n",
    "    max_position_embeddings=2048,\n",
    "    pre_norm=True,\n",
    "    tie_weights=True,\n",
    "    max_seq_len=2048,\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "model = Transformer(config).to(device)\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint_path = \"../Models/best_model_params_80k.pt\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded from {checkpoint_path}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1160a9",
   "metadata": {},
   "source": [
    "### HellaSwag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f660e4c6",
   "metadata": {},
   "source": [
    "#### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edec8c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Rowan/hellaswag\", split=\"validation\")\n",
    "print(f\"Loaded {len(dataset)} examples from HellaSwag validation set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc990ad",
   "metadata": {},
   "source": [
    "#### Evaluation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611bae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and normalize text for HellaSwag.\"\"\"\n",
    "    # HellaSwag specific preprocessing\n",
    "    text = text.strip()\n",
    "    # Add space before if needed for proper tokenization\n",
    "    return text\n",
    "\n",
    "\n",
    "def encode_text(text):\n",
    "    \"\"\"Encode text using the tokenizer.\"\"\"\n",
    "    output = tokenizer.encode(text)\n",
    "    return torch.tensor(output.ids, dtype=torch.long)\n",
    "\n",
    "\n",
    "def compute_perplexity(model, input_ids):\n",
    "    \"\"\"\n",
    "    Compute perplexity for a sequence by calculating the average log likelihood.\n",
    "    Lower perplexity = higher likelihood = better continuation.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        input_ids = input_ids.unsqueeze(0).to(device)  # Add batch dimension\n",
    "        logits = model(input_ids)\n",
    "        \n",
    "        # Shift logits and labels for next token prediction\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = input_ids[:, 1:].contiguous()\n",
    "        \n",
    "        # Calculate log probabilities\n",
    "        log_probs = F.log_softmax(shift_logits, dim=-1)\n",
    "        \n",
    "        # Gather the log probabilities of the actual tokens\n",
    "        token_log_probs = log_probs.gather(2, shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        # Average negative log likelihood\n",
    "        avg_nll = -token_log_probs.mean().item()\n",
    "        \n",
    "        return avg_nll\n",
    "\n",
    "\n",
    "def compute_continuation_score(model, context, continuation):\n",
    "    \"\"\"\n",
    "    Compute the score for a continuation given a context.\n",
    "    Uses perplexity of the full sequence as the scoring metric.\n",
    "    \"\"\"\n",
    "    # Combine context and continuation\n",
    "    full_text = context + \" \" + continuation\n",
    "    \n",
    "    try:\n",
    "        # Encode the full text\n",
    "        input_ids = encode_text(full_text)\n",
    "        \n",
    "        # Skip if sequence is too long\n",
    "        if len(input_ids) > config.max_seq_len:\n",
    "            return float('inf')  # Return worst score\n",
    "        \n",
    "        # Compute perplexity (lower is better)\n",
    "        score = compute_perplexity(model, input_ids)\n",
    "        \n",
    "        return score\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return float('inf')\n",
    "\n",
    "\n",
    "def evaluate_hellaswag_example(model, example):\n",
    "    \"\"\"\n",
    "    Evaluate a single HellaSwag example.\n",
    "    Returns True if model prediction matches the correct label.\n",
    "    \"\"\"\n",
    "    context = preprocess_text(example['ctx'])\n",
    "    endings = [preprocess_text(ending) for ending in example['endings']]\n",
    "    correct_label = int(example['label'])\n",
    "    \n",
    "    # Compute scores for all endings\n",
    "    scores = []\n",
    "    for ending in endings:\n",
    "        score = compute_continuation_score(model, context, ending)\n",
    "        scores.append(score)\n",
    "    \n",
    "    # Predict the ending with lowest perplexity (best fit)\n",
    "    predicted_label = np.argmin(scores)\n",
    "    \n",
    "    return predicted_label == correct_label, predicted_label, correct_label, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36eb26a",
   "metadata": {},
   "source": [
    "#### Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47ec172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hellaswag_benchmark(model, dataset, num_examples=None, save_results=True):\n",
    "    \"\"\"\n",
    "    Run the complete HellaSwag benchmark.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model to evaluate\n",
    "        dataset: HellaSwag dataset\n",
    "        num_examples: Number of examples to evaluate (None = all)\n",
    "        save_results: Whether to save detailed results to file\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with accuracy and detailed results\n",
    "    \"\"\"\n",
    "    if num_examples is None:\n",
    "        num_examples = len(dataset)\n",
    "    else:\n",
    "        num_examples = min(num_examples, len(dataset))\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Evaluating on {num_examples} examples...\")\n",
    "    \n",
    "    for i in tqdm(range(num_examples)):\n",
    "        example = dataset[i]\n",
    "        is_correct, pred_label, true_label, scores = evaluate_hellaswag_example(model, example)\n",
    "        \n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        result = {\n",
    "            'index': i,\n",
    "            'context': example['ctx'],\n",
    "            'endings': example['endings'],\n",
    "            'predicted': int(pred_label),\n",
    "            'correct': int(true_label),\n",
    "            'is_correct': bool(is_correct),\n",
    "            'scores': [float(s) for s in scores]\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        # Print progress every 100 examples\n",
    "        if (i + 1) % 100 == 0:\n",
    "            current_acc = correct / total * 100\n",
    "            print(f\"Progress: {i+1}/{num_examples} | Current Accuracy: {current_acc:.2f}%\")\n",
    "    \n",
    "    accuracy = correct / total * 100\n",
    "    \n",
    "    benchmark_results = {\n",
    "        'accuracy': accuracy,\n",
    "        'correct': correct,\n",
    "        'total': total,\n",
    "        'num_examples': num_examples,\n",
    "        'detailed_results': results\n",
    "    }\n",
    "    \n",
    "    #Save results to file\n",
    "    if save_results:\n",
    "        with open('hellaswag_benchmark_results.json', 'w') as f:\n",
    "            json.dump(benchmark_results, f, indent=2)\n",
    "        print(f\"\\nResults saved to 'hellaswag_benchmark_results.json'\")\n",
    "    \n",
    "    return benchmark_results\n",
    "\n",
    "\n",
    "# Run the benchmark on a subset first (for testing)\n",
    "# Change num_examples=None to evaluate on the full dataset\n",
    "results = run_hellaswag_benchmark(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    num_examples=1024,  # Start with 100 examples, set to None for full evaluation\n",
    "    save_results=True\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"HELLASWAG BENCHMARK RESULTS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Accuracy: {results['accuracy']:.2f}%\")\n",
    "print(f\"Correct: {results['correct']}/{results['total']}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5d0798",
   "metadata": {},
   "source": [
    "#### Analyze Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a51096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some examples of correct and incorrect predictions\n",
    "print(\"=\" * 80)\n",
    "print(\"SAMPLE CORRECT PREDICTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "correct_examples = [r for r in results['detailed_results'] if r['is_correct']]\n",
    "for i, example in enumerate(correct_examples[:3]):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Context: {example['context'][:100]}...\")\n",
    "    print(f\"Predicted ending ({example['predicted']}): {example['endings'][example['predicted']][:80]}...\")\n",
    "    print(f\"Correct answer: {example['correct']}\")\n",
    "    print(f\"Scores (lower=better): {[f'{s:.3f}' for s in example['scores']]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE INCORRECT PREDICTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "incorrect_examples = [r for r in results['detailed_results'] if not r['is_correct']]\n",
    "for i, example in enumerate(incorrect_examples[:3]):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Context: {example['context'][:100]}...\")\n",
    "    print(f\"Predicted ending ({example['predicted']}): {example['endings'][example['predicted']][:80]}...\")\n",
    "    print(f\"Correct ending ({example['correct']}): {example['endings'][example['correct']][:80]}...\")\n",
    "    print(f\"Scores (lower=better): {[f'{s:.3f}' for s in example['scores']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec12ecf6",
   "metadata": {},
   "source": [
    "#### Comparison with Baselines\n",
    "\n",
    "For reference, here are typical HellaSwag accuracy scores:\n",
    "- **Random Chance**: 25% (4 choices)\n",
    "- **GPT-2 (117M)**: ~29-31%\n",
    "- **GPT-2 (1.5B)**: ~40-43%\n",
    "- **GPT-3 (175B)**: ~78-79%\n",
    "- **Human Performance**: ~95%\n",
    "\n",
    "Your model's performance will depend on:\n",
    "1. Model size and architecture\n",
    "2. Training data quality and quantity\n",
    "3. Training duration\n",
    "4. Whether the model has seen similar reasoning tasks during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cc9bae",
   "metadata": {},
   "source": [
    "### ARC "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1da988e",
   "metadata": {},
   "source": [
    "#### Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86df4747",
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_easy = load_dataset(\"allenai/ai2_arc\", \"ARC-Easy\", split=\"test\")\n",
    "arc_challenge = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\", split=\"test\")\n",
    "\n",
    "print(f\"Loaded {len(arc_easy)} examples from ARC-Easy test set\")\n",
    "print(f\"Loaded {len(arc_challenge)} examples from ARC-Challenge test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b36f2b",
   "metadata": {},
   "source": [
    "#### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8087574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and normalize text for ARC.\"\"\"\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def encode_text(text):\n",
    "    \"\"\"Encode text using the tokenizer.\"\"\"\n",
    "    output = tokenizer.encode(text)\n",
    "    return torch.tensor(output.ids, dtype=torch.long)\n",
    "\n",
    "\n",
    "def compute_perplexity(model, input_ids):\n",
    "    \"\"\"\n",
    "    Compute perplexity for a sequence by calculating the average log likelihood.\n",
    "    Lower perplexity = higher likelihood = better answer.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        input_ids = input_ids.unsqueeze(0).to(device)\n",
    "        logits = model(input_ids)\n",
    "        \n",
    "        # Shift logits and labels for next token prediction\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = input_ids[:, 1:].contiguous()\n",
    "        \n",
    "        # Calculate log probabilities\n",
    "        log_probs = F.log_softmax(shift_logits, dim=-1)\n",
    "        \n",
    "        # Gather the log probabilities of the actual tokens\n",
    "        token_log_probs = log_probs.gather(2, shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        # Average negative log likelihood\n",
    "        avg_nll = -token_log_probs.mean().item()\n",
    "        \n",
    "        return avg_nll\n",
    "\n",
    "\n",
    "def compute_answer_score(model, question, answer):\n",
    "    \"\"\"\n",
    "    Compute the score for an answer given a question.\n",
    "    Uses perplexity of the question + answer sequence as the scoring metric.\n",
    "    \"\"\"\n",
    "    # Format as Q&A\n",
    "    full_text = f\"Question: {question} Answer: {answer}\"\n",
    "    \n",
    "    try:\n",
    "        # Encode the full text\n",
    "        input_ids = encode_text(full_text)\n",
    "        \n",
    "        # Skip if sequence is too long\n",
    "        if len(input_ids) > config.max_seq_len:\n",
    "            return float('inf')\n",
    "        \n",
    "        # Compute perplexity (lower is better)\n",
    "        score = compute_perplexity(model, input_ids)\n",
    "        \n",
    "        return score\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return float('inf')\n",
    "\n",
    "\n",
    "def evaluate_arc_example(model, example):\n",
    "    \"\"\"\n",
    "    Evaluate a single ARC example.\n",
    "    Returns True if model prediction matches the correct answer.\n",
    "    \"\"\"\n",
    "    question = preprocess_text(example['question'])\n",
    "    choices = example['choices']\n",
    "    \n",
    "    # Extract choice texts and labels\n",
    "    choice_texts = [preprocess_text(choice) for choice in choices['text']]\n",
    "    choice_labels = choices['label']\n",
    "    correct_answer = example['answerKey']\n",
    "    \n",
    "    # Compute scores for all choices\n",
    "    scores = []\n",
    "    for choice_text in choice_texts:\n",
    "        score = compute_answer_score(model, question, choice_text)\n",
    "        scores.append(score)\n",
    "    \n",
    "    # Predict the choice with lowest perplexity (best fit)\n",
    "    predicted_idx = np.argmin(scores)\n",
    "    predicted_label = choice_labels[predicted_idx]\n",
    "    \n",
    "    return predicted_label == correct_answer, predicted_label, correct_answer, scores, choice_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9d7762",
   "metadata": {},
   "source": [
    "#### Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0c6ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_arc_benchmark(model, dataset, dataset_name, num_examples=None, save_results=True):\n",
    "    \"\"\"\n",
    "    Run the complete ARC benchmark.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model to evaluate\n",
    "        dataset: ARC dataset (Easy or Challenge)\n",
    "        dataset_name: Name of the dataset for reporting\n",
    "        num_examples: Number of examples to evaluate (None = all)\n",
    "        save_results: Whether to save detailed results to file\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with accuracy and detailed results\n",
    "    \"\"\"\n",
    "    if num_examples is None:\n",
    "        num_examples = len(dataset)\n",
    "    else:\n",
    "        num_examples = min(num_examples, len(dataset))\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Evaluating {dataset_name} on {num_examples} examples...\")\n",
    "    \n",
    "    for i in tqdm(range(num_examples)):\n",
    "        example = dataset[i]\n",
    "        is_correct, pred_label, true_label, scores, choice_labels = evaluate_arc_example(model, example)\n",
    "        \n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        # Store detailed results\n",
    "        result = {\n",
    "            'index': i,\n",
    "            'question': example['question'],\n",
    "            'choices': {\n",
    "                'text': example['choices']['text'],\n",
    "                'label': example['choices']['label']\n",
    "            },\n",
    "            'predicted': pred_label,\n",
    "            'correct': true_label,\n",
    "            'is_correct': bool(is_correct),\n",
    "            'scores': [float(s) for s in scores]\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        # Print progress every 50 examples\n",
    "        if (i + 1) % 50 == 0:\n",
    "            current_acc = correct / total * 100\n",
    "            print(f\"Progress: {i+1}/{num_examples} | Current Accuracy: {current_acc:.2f}%\")\n",
    "    \n",
    "    accuracy = correct / total * 100\n",
    "    \n",
    "    benchmark_results = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'accuracy': accuracy,\n",
    "        'correct': correct,\n",
    "        'total': total,\n",
    "        'num_examples': num_examples,\n",
    "        'detailed_results': results\n",
    "    }\n",
    "    \n",
    "    # Save results to file\n",
    "    if save_results:\n",
    "        filename = f'arc_{dataset_name.lower().replace(\"-\", \"_\")}_results.json'\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(benchmark_results, f, indent=2)\n",
    "        print(f\"\\nResults saved to '{filename}'\")\n",
    "    \n",
    "    return benchmark_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7859d05",
   "metadata": {},
   "source": [
    "#### Evaluate ARC-Easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699f0a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change num_examples=None to evaluate on the full dataset\n",
    "easy_results = run_arc_benchmark(\n",
    "    model=model,\n",
    "    dataset=arc_easy,\n",
    "    dataset_name=\"ARC-Easy\",\n",
    "    num_examples=None,  # Start with 100 examples, set to None for full evaluation\n",
    "    save_results=True\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"ARC-EASY BENCHMARK RESULTS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Accuracy: {easy_results['accuracy']:.2f}%\")\n",
    "print(f\"Correct: {easy_results['correct']}/{easy_results['total']}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a311d02b",
   "metadata": {},
   "source": [
    "#### Evaluate ARC-Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8562ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change num_examples=None to evaluate on the full dataset\n",
    "challenge_results = run_arc_benchmark(\n",
    "    model=model,\n",
    "    dataset=arc_challenge,\n",
    "    dataset_name=\"ARC-Challenge\",\n",
    "    num_examples=None,  # Start with 100 examples, set to None for full evaluation\n",
    "    save_results=True\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"ARC-CHALLENGE BENCHMARK RESULTS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Accuracy: {challenge_results['accuracy']:.2f}%\")\n",
    "print(f\"Correct: {challenge_results['correct']}/{challenge_results['total']}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d86833",
   "metadata": {},
   "source": [
    "#### Comparison with Baselines\n",
    "\n",
    "For reference, here are typical ARC accuracy scores:\n",
    "\n",
    "#### ARC-Easy:\n",
    "- **Random Chance**: ~25% (4 choices on average)\n",
    "- **GPT-2 (117M)**: ~40-45%\n",
    "- **GPT-2 (1.5B)**: ~55-60%\n",
    "- **GPT-3 (175B)**: ~70-75%\n",
    "- **Human Performance**: ~90%\n",
    "\n",
    "#### ARC-Challenge:\n",
    "- **Random Chance**: ~25% (4 choices on average)\n",
    "- **GPT-2 (117M)**: ~20-25%\n",
    "- **GPT-2 (1.5B)**: ~30-35%\n",
    "- **GPT-3 (175B)**: ~50-55%\n",
    "- **Human Performance**: ~85%\n",
    "\n",
    "#### What affects performance:\n",
    "1. **Model size**: Larger models typically perform better on reasoning tasks\n",
    "2. **Training data**: Models trained on more diverse scientific/educational content perform better\n",
    "3. **Architecture**: Better attention mechanisms help with understanding context\n",
    "4. **Training duration**: More training generally improves reasoning capabilities\n",
    "\n",
    "The ARC-Challenge set is significantly harder than ARC-Easy, requiring deeper reasoning and scientific knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891826b7",
   "metadata": {},
   "source": [
    "### PIQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aded3a",
   "metadata": {},
   "source": [
    "#### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542d1a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ybisk/piqa\", split=\"validation\")\n",
    "print(f\"Loaded {len(dataset)} examples from PIQA validation set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270ad59e",
   "metadata": {},
   "source": [
    "#### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c767c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and normalize text for PIQA.\"\"\"\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def encode_text(text):\n",
    "    \"\"\"Encode text using the tokenizer.\"\"\"\n",
    "    output = tokenizer.encode(text)\n",
    "    return torch.tensor(output.ids, dtype=torch.long)\n",
    "\n",
    "\n",
    "def compute_perplexity(model, input_ids):\n",
    "    \"\"\"\n",
    "    Compute perplexity for a sequence by calculating the average log likelihood.\n",
    "    Lower perplexity = higher likelihood = better solution.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        input_ids = input_ids.unsqueeze(0).to(device)\n",
    "        logits = model(input_ids)\n",
    "        \n",
    "        # Shift logits and labels for next token prediction\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = input_ids[:, 1:].contiguous()\n",
    "        \n",
    "        # Calculate log probabilities\n",
    "        log_probs = F.log_softmax(shift_logits, dim=-1)\n",
    "        \n",
    "        # Gather the log probabilities of the actual tokens\n",
    "        token_log_probs = log_probs.gather(2, shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        # Average negative log likelihood\n",
    "        avg_nll = -token_log_probs.mean().item()\n",
    "        \n",
    "        return avg_nll\n",
    "\n",
    "\n",
    "def compute_solution_score(model, goal, solution):\n",
    "    \"\"\"\n",
    "    Compute the score for a solution given a goal.\n",
    "    Uses perplexity of the goal + solution sequence as the scoring metric.\n",
    "    \"\"\"\n",
    "    # Format as Goal + Solution\n",
    "    full_text = f\"Goal: {goal} Solution: {solution}\"\n",
    "    \n",
    "    try:\n",
    "        # Encode the full text\n",
    "        input_ids = encode_text(full_text)\n",
    "        \n",
    "        # Skip if sequence is too long\n",
    "        if len(input_ids) > config.max_seq_len:\n",
    "            return float('inf')\n",
    "        \n",
    "        # Compute perplexity (lower is better)\n",
    "        score = compute_perplexity(model, input_ids)\n",
    "        \n",
    "        return score\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return float('inf')\n",
    "\n",
    "\n",
    "def evaluate_piqa_example(model, example):\n",
    "    \"\"\"\n",
    "    Evaluate a single PIQA example.\n",
    "    Returns True if model prediction matches the correct label.\n",
    "    \"\"\"\n",
    "    goal = preprocess_text(example['goal'])\n",
    "    solution1 = preprocess_text(example['sol1'])\n",
    "    solution2 = preprocess_text(example['sol2'])\n",
    "    correct_label = int(example['label'])\n",
    "    \n",
    "    # Compute scores for both solutions\n",
    "    score1 = compute_solution_score(model, goal, solution1)\n",
    "    score2 = compute_solution_score(model, goal, solution2)\n",
    "    \n",
    "    scores = [score1, score2]\n",
    "    \n",
    "    # Predict the solution with lowest perplexity (best fit)\n",
    "    predicted_label = np.argmin(scores)\n",
    "    \n",
    "    return predicted_label == correct_label, predicted_label, correct_label, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2a9a2d",
   "metadata": {},
   "source": [
    "#### Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4abbd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_piqa_benchmark(model, dataset, num_examples=None, save_results=True):\n",
    "    \"\"\"\n",
    "    Run the complete PIQA benchmark.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model to evaluate\n",
    "        dataset: PIQA dataset\n",
    "        num_examples: Number of examples to evaluate (None = all)\n",
    "        save_results: Whether to save detailed results to file\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with accuracy and detailed results\n",
    "    \"\"\"\n",
    "    if num_examples is None:\n",
    "        num_examples = len(dataset)\n",
    "    else:\n",
    "        num_examples = min(num_examples, len(dataset))\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Evaluating on {num_examples} examples...\")\n",
    "    \n",
    "    for i in tqdm(range(num_examples)):\n",
    "        example = dataset[i]\n",
    "        is_correct, pred_label, true_label, scores = evaluate_piqa_example(model, example)\n",
    "        \n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        # Store detailed results\n",
    "        result = {\n",
    "            'index': i,\n",
    "            'goal': example['goal'],\n",
    "            'solutions': [example['sol1'], example['sol2']],\n",
    "            'predicted': int(pred_label),\n",
    "            'correct': int(true_label),\n",
    "            'is_correct': bool(is_correct),\n",
    "            'scores': [float(s) for s in scores]\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        # Print progress every 100 examples\n",
    "        if (i + 1) % 100 == 0:\n",
    "            current_acc = correct / total * 100\n",
    "            print(f\"Progress: {i+1}/{num_examples} | Current Accuracy: {current_acc:.2f}%\")\n",
    "    \n",
    "    accuracy = correct / total * 100\n",
    "    \n",
    "    benchmark_results = {\n",
    "        'accuracy': accuracy,\n",
    "        'correct': correct,\n",
    "        'total': total,\n",
    "        'num_examples': num_examples,\n",
    "        'detailed_results': results\n",
    "    }\n",
    "    \n",
    "    # Save results to file\n",
    "    if save_results:\n",
    "        with open('piqa_benchmark_results.json', 'w') as f:\n",
    "            json.dump(benchmark_results, f, indent=2)\n",
    "        print(f\"\\nResults saved to 'piqa_benchmark_results.json'\")\n",
    "    \n",
    "    return benchmark_results\n",
    "\n",
    "\n",
    "# Run the benchmark on a subset first (for testing)\n",
    "# Change num_examples=None to evaluate on the full dataset\n",
    "results = run_piqa_benchmark(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    num_examples=100,  # Start with 100 examples, set to None for full evaluation\n",
    "    save_results=True\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"PIQA BENCHMARK RESULTS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Accuracy: {results['accuracy']:.2f}%\")\n",
    "print(f\"Correct: {results['correct']}/{results['total']}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0967c3d7",
   "metadata": {},
   "source": [
    "#### Comparison with Baselines\n",
    "\n",
    "For reference, here are typical PIQA accuracy scores:\n",
    "\n",
    "- **Random Chance**: 50% (2 choices)\n",
    "- **GPT-2 (117M)**: ~63-65%\n",
    "- **GPT-2 (1.5B)**: ~70-73%\n",
    "- **GPT-3 (175B)**: ~81-82%\n",
    "- **RoBERTa Large**: ~79%\n",
    "- **Human Performance**: ~95%\n",
    "\n",
    "#### What is PIQA?\n",
    "\n",
    "PIQA (Physical Interaction QA) tests a model's understanding of physical commonsense. Each question presents:\n",
    "- A **goal** (e.g., \"To separate egg whites from the yolk\")\n",
    "- Two **solutions** (e.g., using different physical methods)\n",
    "\n",
    "The model must choose which solution is physically plausible and practical.\n",
    "\n",
    "#### What affects performance:\n",
    "\n",
    "1. **Physical Commonsense**: Understanding how objects interact in the real world\n",
    "2. **Practical Knowledge**: Knowing everyday solutions to common problems\n",
    "3. **Model Size**: Larger models tend to have better physical reasoning\n",
    "4. **Training Data**: Models trained on diverse practical/how-to content perform better\n",
    "5. **Reasoning Ability**: Understanding cause and effect in physical scenarios\n",
    "\n",
    "PIQA is particularly challenging because it requires:\n",
    "- Understanding of physics and material properties\n",
    "- Practical experience with everyday objects\n",
    "- Ability to reason about physical interactions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
