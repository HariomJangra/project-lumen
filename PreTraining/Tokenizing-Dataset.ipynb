{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0972cf7",
   "metadata": {},
   "source": [
    "### Importing and Installing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cfca9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7560e5e",
   "metadata": {},
   "source": [
    "### Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8bf18e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e032c474074a2ab8d9463651ba5ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce82208ca06c43b9af5680326ab598ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"HariomJangra/Lumen-PreTraining\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827776d0",
   "metadata": {},
   "source": [
    "### Load Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c971dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"LumenTokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c4634a",
   "metadata": {},
   "source": [
    "### Tokenizing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea6c09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 6017613it [45:15, 423.80it/s] "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "chunk_size_samples = 2_000_000 \n",
    "chunk_counter = 0\n",
    "eos_id = tokenizer.encode(\"<|endoftext|>\", add_special_tokens=False).ids[0]\n",
    "\n",
    "current_chunk = []\n",
    "current_samples = 0\n",
    "\n",
    "for item in tqdm(dataset, desc=\"Tokenizing\"):\n",
    "    tokens = tokenizer.encode(item[\"text\"], add_special_tokens=False).ids\n",
    "    current_chunk.extend(tokens)\n",
    "    current_chunk.append(eos_id)\n",
    "    \n",
    "    current_samples += 1\n",
    "    if current_samples >= chunk_size_samples:\n",
    "        np.save(f\"tokens_chunk_{chunk_counter}.npy\", np.array(current_chunk, dtype=np.int32))\n",
    "        print(f\"Saved chunk {chunk_counter} with {len(current_chunk)} tokens ({current_samples} samples)\")\n",
    "        chunk_counter += 1\n",
    "        current_chunk = []\n",
    "        current_samples = 0\n",
    "\n",
    "if current_chunk:\n",
    "    np.save(f\"tokens_chunk_{chunk_counter}.npy\", np.array(current_chunk, dtype=np.int32))\n",
    "    print(f\"Saved final chunk {chunk_counter} with {len(current_chunk)} tokens ({current_samples} samples)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3605ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged chunk saved with 4077295745 tokens\n"
     ]
    }
   ],
   "source": [
    "chunks_to_merge = [\"tokens_chunk_0.npy\", \"tokens_chunk_1.npy\", \"tokens_chunk_2.npy\", \"tokens_chunk_3.npy\", \"tokens_chunk_4.npy\"]\n",
    "\n",
    "all_tokens = []\n",
    "\n",
    "# Load and append each chunk\n",
    "for chunk_file in chunks_to_merge:\n",
    "    arr = np.load(chunk_file)\n",
    "    all_tokens.append(arr)\n",
    "\n",
    "# Concatenate all tokens into one array\n",
    "merged_tokens = np.concatenate(all_tokens)\n",
    "\n",
    "# Save as a new merged chunk\n",
    "np.save(\"TokenizedDataSet.npy\", all_tokens)\n",
    "print(f\"Merged chunk saved with {len(all_tokens)} tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf975522",
   "metadata": {},
   "source": [
    "### Train and Validation Splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71502032",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = np.load(\"TokenizedDataSet.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1495d054",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = int(0.9*len(all_tokens))\n",
    "train_split = all_tokens[:split_ratio]\n",
    "val_split = all_tokens[split_ratio:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5d78c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Training and Validation Data\n",
      "Training and Validation Data Saved!\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving Training and Validation Data\")\n",
    "np.save(\"train_split.npy\", train_split)\n",
    "np.save(\"val_split.npy\", val_split)\n",
    "print(\"Training and Validation Data Saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
